{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdflib in c:\\users\\mvass\\anaconda3\\lib\\site-packages (4.2.2)\n",
      "Requirement already satisfied: isodate in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from rdflib) (0.6.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from rdflib) (2.4.0)\n",
      "Requirement already satisfied: six in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from isodate->rdflib) (1.12.0)\n",
      "Requirement already satisfied: bs4 in c:\\users\\mvass\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from bs4) (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (1.8)\n",
      "Requirement already satisfied: SPARQLWrapper in c:\\users\\mvass\\anaconda3\\lib\\site-packages (1.8.2)\n",
      "Requirement already satisfied: rdflib>=4.0 in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from SPARQLWrapper) (4.2.2)\n",
      "Requirement already satisfied: isodate in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from rdflib>=4.0->SPARQLWrapper) (0.6.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from rdflib>=4.0->SPARQLWrapper) (2.4.0)\n",
      "Requirement already satisfied: six in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from isodate->rdflib>=4.0->SPARQLWrapper) (1.12.0)\n",
      "Requirement already satisfied: pywikibot in c:\\users\\mvass\\anaconda3\\lib\\site-packages (3.0.dev0)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from pywikibot) (2.22.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from requests>=2.20.0->pywikibot) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from requests>=2.20.0->pywikibot) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from requests>=2.20.0->pywikibot) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mvass\\anaconda3\\lib\\site-packages (from requests>=2.20.0->pywikibot) (2019.9.11)\n"
     ]
    }
   ],
   "source": [
    "#Installs necessary modules\n",
    "!pip install rdflib\n",
    "!pip install bs4\n",
    "!pip install SPARQLWrapper\n",
    "!pip install pywikibot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import RDFLib module and associated classes\n",
    "import rdflib as rb\n",
    "from rdflib import Literal, BNode, URIRef, Graph, Namespace\n",
    "from rdflib.namespace import RDF, OWL, RDFS, FOAF, XSD, DC\n",
    "\n",
    "#Import SPARQL\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON, CSV\n",
    "\n",
    "#Import spaCy module\n",
    "import spacy as sp\n",
    "\n",
    "#Import pre-installed modules for various analysis\n",
    "import urllib.request\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "#Import BeautifulSoup for webscraping\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Namespace definitions\n",
    "pr = rb.Namespace(\"http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#\")\n",
    "sch = rb.Namespace(\"http://schema.org/\")\n",
    "xsd = rb.Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "rdfs = rb.Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "owl = rb.Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "wdt = rb.Namespace(\"https://www.wikidata.org/wiki/Property_talk:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search For Information\n",
    "These entity functions look for different objects associated to a class.  If the object is found in the article, it is to be instatiated and appended to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entity functions\n",
    "#Each function looks for the desired information to be extracted.\n",
    "\n",
    "def facultySearch(fac_token):\n",
    "    if fac_token.label_ == \"PERSON\" and (\" \" in fac_token.text):\n",
    "        fac_token = fac_token.text\n",
    "        fac_token = fac_token.replace(\"</strong\",\"\")\n",
    "        fac_token = fac_token.replace(\"</p\",\"\")\n",
    "        fac_token = fac_token.replace(\">\",\"\")\n",
    "        person.append(fac_token)\n",
    "        person.append(\"Mark S. Fox\")\n",
    "\n",
    "def dateSearch(dat_token):\n",
    "    if dat_token.label_ == \"DATE\" and any(k in str(dat_token) for k in date_words):\n",
    "        datetime.append(dat_token.text)\n",
    "\n",
    "def citySearch(cit_token):\n",
    "    try:\n",
    "        #This file should be downloaded and added to the interpretor's directory.  If not, the search will not be as accurate.\n",
    "        #This is a giant list of existing Provinces and States all over the world.  This will help condense the GPE list.\n",
    "        #Please do not change the file name!\n",
    "        with open('2019-1 SubdivisionCodes.txt') as f:\n",
    "            if (cit_token.label_ == \"GPE\") and (cit_token.text not in cty_text) and (not any(k in str(cit_token) for k in num_words)) and (cit_token.text not in f.read()):\n",
    "                city.append(cit_token.text)\n",
    "                city.remove(\"Surgery\")\n",
    "    except:\n",
    "        if (cit_token.label_ == \"GPE\") and (cit_token.text not in cty_text) and (not any(k in str(cit_token) for k in num_words)):\n",
    "            city.append(cit_token.text)\n",
    "            \n",
    "def countrySearch(cty_token):\n",
    "    if (cty_token.label_ == \"GPE\") and (cty_token.text in cty_text):\n",
    "        country.append(cty_token.text)\n",
    "        \n",
    "def departmentSearch(dep_token):\n",
    "    if (dep_token.label_== \"ORG\") and (dep_token.text in dep_text) and (not any(k in str(dep_token) for k in null_words)):\n",
    "        department.append(dep_token.text)\n",
    "        department.append(\"Computer science\")\n",
    "        if \">\" in department:\n",
    "            department.remove(\">\")\n",
    "        if \"Cell\" in department:\n",
    "            department.remove(\"Cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Data to KnowledgeGraph\n",
    "This is for the first knowledge graph from Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once information is extracted from each search, add it to the UofT knowledgegraph\n",
    "def addObject():\n",
    "    \n",
    "    master_list = [\"MENTION--TYPE--CLASS\\n\",\"<b>MENTION</b>--<b>TYPE</b>--<b>CLASS</b>\\n\"]\n",
    "    \n",
    "    #Adding mentions -------------------------------------------------------------\n",
    "    g.add((pr.Mention, RDF.type, owl.Class))\n",
    "    g.add((pr.Mention, pr.source, pr.Source))\n",
    "    g.add((pr.Mention, pr.person, pr.Person))\n",
    "    g.add((pr.Mention, pr.topic, xsd.string))\n",
    "    g.add((pr.Mention, pr.department, pr.Department))\n",
    "    g.add((pr.Mention, pr.date, xsd.datetime))\n",
    "    \n",
    "    g.add((pr.Source, sch.name, xsd.string))\n",
    "    g.add((pr.Source, sch.description, xsd.string))\n",
    "    g.add((pr.Source, pr.inCity, sch.City))\n",
    "    g.add((pr.Source, sch.countryOfOrigin, sch.Country))\n",
    "    \n",
    "    g.add((pr.Department, rdfs.subClassOf, sch.Organization))\n",
    "    g.add((pr.Department, sch.name, xsd.string))\n",
    "    g.add((pr.Department, sch.description, xsd.string))\n",
    "    \n",
    "    for s,p,o in g:\n",
    "        gr = str(s) + \"--\" + str(p) + \"--\" + str(o) + \"\\n\"\n",
    "        master_list.append(gr)\n",
    "        \n",
    "    #Adding mention---------------------------------------------------------------\n",
    "    m = 0\n",
    "    for i in list(dict.fromkeys(person)): #Adding person\n",
    "        m += 1\n",
    "        add_person = Literal(i)\n",
    "        g.add((pr[\"p\" + str(m)], pr.name,add_person))\n",
    "        g.add((pr.Person, RDF.type,pr[\"p\" + str(m)]))\n",
    "        m_list.append(m)\n",
    "\n",
    "    g.add((pr.m1, pr.topic, Literal(topic_string))) #Adding topic description\n",
    "    \n",
    "    m = 0\n",
    "    for i in list(dict.fromkeys(datetime)):   #Adding all dates mentioned\n",
    "        m += 1\n",
    "        add_date = Literal(i)\n",
    "        g.add((pr[\"date\" + str(m)], pr.date, add_date))\n",
    "        m_list.append(m)\n",
    "    \n",
    "    #Adding source---------------------------------------------------------------\n",
    "    g.add((pr.s1, sch.name, Literal(source)))\n",
    "    g.add((pr.Source, RDF.type, pr.s1))\n",
    "    \n",
    "    s_list.append(1)\n",
    "    g.add((pr.s2, sch.name, Literal(\"CBC News\")))\n",
    "    g.add((pr.Source, RDF.type, pr.s2))\n",
    "    \n",
    "    s_list.append(2)\n",
    "    \n",
    "    s = 0\n",
    "    for i in list(dict.fromkeys(city)): #Adding in city\n",
    "        s += 1\n",
    "        add_city = Literal(i)\n",
    "        g.add((pr[\"city\" + str(s)], pr.name, add_city))\n",
    "        g.add((pr.City,RDF.type,pr[\"city\" + str(s)]))\n",
    "        s_list.append(s)\n",
    "\n",
    "    s = 0\n",
    "    for i in list(dict.fromkeys(country)): #Adding in countries\n",
    "        s += 1\n",
    "        add_country = Literal(i)\n",
    "        g.add((pr[\"country\" + str(s)], sch.name, add_country))\n",
    "        g.add((pr.Country,RDF.type, pr[\"country\" + str(s)]))\n",
    "        s_list.append(s)\n",
    "        \n",
    "    #Adding Department-----------------------------------------------------------\n",
    "    d = 0\n",
    "    for i in list(dict.fromkeys(department)): #Adding department\n",
    "        d += 1\n",
    "        add_department = Literal(i)\n",
    "        g.add((pr[\"d\" + str(d)], sch.name, add_department))\n",
    "        g.add((pr.Department, RDF.type, pr[\"d\" + str(d)]))\n",
    "        d_list.append(d)\n",
    "    \n",
    "    #Converting knowledge graph to text -----------------------------------------\n",
    "    kg_list = []\n",
    "    \n",
    "    for s,p,o in g:\n",
    "        gr = str(s) + \"--\" + str(p) + \"--\" + str(o) + \"\\n\"\n",
    "        kg_list.append(gr)\n",
    "    \n",
    "    if len(m_list) > 0:\n",
    "        for i in range(1,max(m_list)+1):\n",
    "            master_list.append(\"<b>PERSON \" + str(i) + \"</b>--<b>PREDICATE</b>--<b>OBJECT</b>\\n\")\n",
    "            for kg in kg_list:\n",
    "                if \"p\"+str(i) in kg:\n",
    "                    master_list.append(kg)\n",
    "\n",
    "    if len(s_list) > 0:\n",
    "        for i in range(1,max(s_list)+1):\n",
    "            master_list.append(\"<b>CITY SOURCE \" + str(i) + \"</b>--<b>PREDICATE</b>--<b>OBJECT</b>\\n\")\n",
    "                            \n",
    "            for kg in kg_list:\n",
    "                if \"city\"+str(i) in kg:\n",
    "                    master_list.append(kg)\n",
    "    \n",
    "    if len(s_list) > 0:\n",
    "        for i in range(1,max(s_list)+1):\n",
    "            master_list.append(\"<b>COUNTRY SOURCE \" + str(i) + \"</b>--<b>PREDICATE</b>--<b>WEB SOURCE</b>\\n\")    \n",
    "            for kg in kg_list:\n",
    "                if \"country\"+str(i) in kg:\n",
    "                    master_list.append(kg)\n",
    "                    \n",
    "    if len(s_list) > 0:\n",
    "        for i in range(1,max(s_list)+1):\n",
    "            master_list.append(\"<b>WEB SOURCE \" + str(i) + \"</b>--<b>PREDICATE</b>--<b>WEB SOURCE</b>\\n\")        \n",
    "            for kg in kg_list:\n",
    "                if \"s\"+str(i) in kg:\n",
    "                    master_list.append(kg)\n",
    "    \n",
    "    if len(d_list) > 0:\n",
    "        for i in range(1,max(d_list)+1):\n",
    "            master_list.append(\"<b>DEPARTMENT \" + str(i) + \"</b>--<b>PREDCIATE</b>--<b>ORGANIZATION</b>\\n\")\n",
    "            for kg in kg_list:\n",
    "                if \"d\"+str(i) in kg:\n",
    "                    master_list.append(kg)\n",
    "\n",
    "    gr = ''.join(master_list)\n",
    "    print(gr)\n",
    "    \n",
    "    createTable(gr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create HTML Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTable(graphtotext):\n",
    "\n",
    "    def wrap(a, tag):\n",
    "        \"Wraps in <td> tag the a\"\n",
    "        tag1 = tag\n",
    "        if tag == \"table\":\n",
    "            tag1 = \"table border = 2\"\n",
    "        if tag == \"td\" and a.strip().replace(\".\", \"\").isdigit():\n",
    "            tag1 = \"td style=\\\"text-align:right\\\"\"\n",
    "        return f\"<{tag1}>{a}</{tag}>\"\n",
    "\n",
    "    def split(tab):\n",
    "        tab = tab.split(\"\\n\")\n",
    "        for n, row in enumerate(tab):\n",
    "            tab[n] = row.split(\"--\")\n",
    "        return tab\n",
    "\n",
    "    def table(tab):\n",
    "        html = ''  # contain html\n",
    "        for n, x in enumerate(tab):\n",
    "            for a in x:\n",
    "                html += wrap(a, \"td\")\n",
    "            html += \"<tr>\"\n",
    "        html = wrap(html, \"table\")\n",
    "        return html\n",
    "\n",
    "    data = table(split(graphtotext)[1:-1])\n",
    "\n",
    "    with open(\"uoftknowledgegraph.html\", \"w\", encoding = \"utf-8\") as filehtml:\n",
    "        filehtml.write(data)\n",
    "\n",
    "    os.system(\"uoftknowledgegraph.html\")\n",
    "#     convertCodes()\n",
    "\n",
    "def createExtendedTable(graphtotext):\n",
    "\n",
    "    def wrap(a, tag):\n",
    "        \"Wraps in <td> tag the a\"\n",
    "        tag1 = tag\n",
    "        if tag == \"table\":\n",
    "            tag1 = \"table border = 2\"\n",
    "        if tag == \"td\" and a.strip().replace(\".\", \"\").isdigit():\n",
    "            tag1 = \"td style=\\\"text-align:right\\\"\"\n",
    "        return f\"<{tag1}>{a}</{tag}>\"\n",
    "\n",
    "    def split(tab):\n",
    "        tab = tab.split(\"\\n\")\n",
    "        for n, row in enumerate(tab):\n",
    "            tab[n] = row.split(\"--\")\n",
    "        return tab\n",
    "\n",
    "    def table(tab):\n",
    "        html = ''  # contain html\n",
    "        for n, x in enumerate(tab):\n",
    "            for a in x:\n",
    "                html += wrap(a, \"td\")\n",
    "            html += \"<tr>\"\n",
    "        html = wrap(html, \"table\")\n",
    "        return html\n",
    "\n",
    "    data = table(split(graphtotext)[1:-1])\n",
    "\n",
    "    with open(\"uoftknowledgegraphextended.html\", \"w\", encoding = \"utf-8\") as filehtml:\n",
    "        filehtml.write(data)\n",
    "\n",
    "    os.system(\"uoftknowledgegraphextended.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiData Defined Queries\n",
    "- Defined queries for cities, countries, sources, persons, departments and topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personQuery(x,y,g_expand):\n",
    "    from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "    import json\n",
    "    if x == \"Q4662529\":\n",
    "        x = \"Q29921988\"\n",
    "    for p in [\"P21\",\"P106\",\"P69\",\"P569\",\"P31\",\"P27\"]:\n",
    "        endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "        \n",
    "        if p == \"P21\":\n",
    "            pl = \"sex\"\n",
    "        elif p == \"P106\":\n",
    "            pl = \"occupation\"\n",
    "        elif p == \"P69\":\n",
    "            pl = \"educatedAt\"\n",
    "        elif p == \"P569\":\n",
    "            pl = \"dateOfBirth\"\n",
    "        elif p == \"P27\":\n",
    "            pl = \"citizenship\"\n",
    "        else:\n",
    "            pl = \"instanceOf\"\n",
    "        \n",
    "        query = (\"#Continents, countries, regions and capitals\\n\"\n",
    "        \"#defaultView:Tree\\n\"\n",
    "        \"SELECT ?valLabel ?val2Label\\n\"\n",
    "        \"WHERE\\n\" \n",
    "        \"{\\n\"         \n",
    "        \"wd:\"+x+\" wdt:\"+p+\" ?val.\\n\"\n",
    "        \"SERVICE wikibase:label {\\n\"\n",
    "        'bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\".'\n",
    "        \"}\"\n",
    "        \"}\")\n",
    "\n",
    "        sparql = SPARQLWrapper(endpoint_url, agent = \"ChicoBot Test agent\")\n",
    "        sparql.setQuery(query)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        continent_results = sparql.query().convert()\n",
    "\n",
    "        result = str(continent_results[\"results\"][\"bindings\"])\n",
    "        result = str(result[result.find(\"'value': '\")+len(\"'value': '\"):result.rfind(\"'\")])\n",
    "\n",
    "        if result != \"\":\n",
    "            print(result)\n",
    "        else:\n",
    "            result = '<font color=\"red\"><i>MISSING DATA</i></font>'\n",
    "\n",
    "        for s,p,o in g:\n",
    "            if y in o:\n",
    "                g_expand.add((Literal(s),sch.person,Literal(y)))\n",
    "                g_expand.add((Literal(s),wdt[pl],Literal(result)))\n",
    "    return g_expand\n",
    "\n",
    "\n",
    "def cityQuery(x,y,g_expand):\n",
    "    from SPARQLWrapper import SPARQLWrapper, JSON, CSV\n",
    "    import json\n",
    "    \n",
    "    for p in [\"P1376\",\"P1082\",\"P6\",\"P31\",\"P138\"]:\n",
    "        endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "        \n",
    "        if p == \"P1376\":\n",
    "            pl = \"capitalOf\"\n",
    "        elif p == \"P1082\":\n",
    "            pl = \"population\"\n",
    "        elif p == \"P31\":\n",
    "            pl = \"instanceOf\"\n",
    "        elif p == \"P138\":\n",
    "            pl = \"namedAfter\"\n",
    "        else:\n",
    "            pl = \"cityMayor\"\n",
    "        \n",
    "        query = (\"#Continents, countries, regions and capitals\\n\"\n",
    "        \"#defaultView:Tree\\n\"\n",
    "        \"SELECT ?valLabel ?val2Label\\n\"\n",
    "        \"WHERE\\n\" \n",
    "        \"{\\n\"\n",
    "        \"wd:\"+x+\" wdt:\"+p+\" ?val.\\n\"\n",
    "        \"SERVICE wikibase:label {\\n\"\n",
    "        'bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\".'\n",
    "        \"}\"\n",
    "        \"}\")\n",
    "\n",
    "        sparql = SPARQLWrapper(endpoint_url, agent = \"ChicoBot Test agent\")\n",
    "        sparql.setQuery(query)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        continent_results = sparql.query().convert()\n",
    "\n",
    "        result = str(continent_results[\"results\"][\"bindings\"])\n",
    "        result = str(result[result.find(\"'value': '\")+len(\"'value': '\"):result.rfind(\"'\")])\n",
    "\n",
    "        if result != \"\":\n",
    "            print(result)\n",
    "        else:\n",
    "            result = '<font color=\"red\"><i>MISSING DATA</i></font>'\n",
    "\n",
    "        for s,p,o in g:\n",
    "            if y in o:\n",
    "                g_expand.add((Literal(s),sch.inCity,Literal(y)))\n",
    "                g_expand.add((Literal(s),wdt[pl],Literal(result)))\n",
    "    return g_expand\n",
    "\n",
    "\n",
    "def countryQuery(x,y,g_expand):\n",
    "    from SPARQLWrapper import SPARQLWrapper, JSON, CSV\n",
    "    import json\n",
    "    \n",
    "    for p in [\"P30\",\"P36\",\"P571\",\"P31\",\"P2250\"]:\n",
    "        endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "        \n",
    "        if p == \"P30\":\n",
    "            pl = \"continent\"\n",
    "        elif p == \"P36\":\n",
    "            pl = \"capitalCity\"\n",
    "        elif p == \"P31\":\n",
    "            pl = \"instanceOf\"\n",
    "        elif p == \"P2250\":\n",
    "            pl = \"lifeExpectancy\"\n",
    "        else:\n",
    "            pl = \"inceptionDate\"\n",
    "\n",
    "        query = (\"#Continents, countries, regions and capitals\\n\"\n",
    "        \"#defaultView:Tree\\n\"\n",
    "        \"SELECT ?valLabel ?val2Label\\n\"\n",
    "        \"WHERE\\n\" \n",
    "        \"{\\n\"\n",
    "        \"wd:\"+x+\" wdt:\"+p+\" ?val.\\n\"\n",
    "        \"SERVICE wikibase:label {\\n\"\n",
    "        'bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\".'\n",
    "        \"}\"\n",
    "        \"}\")\n",
    "\n",
    "        sparql = SPARQLWrapper(endpoint_url, agent = \"ChicoBot Test agent\")\n",
    "        sparql.setQuery(query)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        continent_results = sparql.query().convert()\n",
    "\n",
    "        result = str(continent_results[\"results\"][\"bindings\"])\n",
    "        result = str(result[result.find(\"'value': '\")+len(\"'value': '\"):result.rfind(\"'\")])\n",
    "\n",
    "        if result != \"\":\n",
    "            print(result)\n",
    "        else:\n",
    "            result = '<font color=\"red\"><i>MISSING DATA</i></font>'\n",
    "\n",
    "        for s,p,o in g:\n",
    "            if y in o:\n",
    "                g_expand.add((Literal(s),sch.countryOfOrigin,Literal(y)))\n",
    "                g_expand.add((Literal(s),wdt[pl],Literal(result)))\n",
    "    return g_expand\n",
    "\n",
    "\n",
    "def sourceQuery(x,y,g_expand):\n",
    "    from SPARQLWrapper import SPARQLWrapper, JSON, CSV\n",
    "    import json\n",
    "    \n",
    "    for p in [\"P159\",\"P31\",\"P452\",\"P112\",\"P856\"]:\n",
    "        endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "        \n",
    "        if p == \"P159\":\n",
    "            pl = \"headquartersLocatedIn\"\n",
    "        elif p == \"P31\":\n",
    "            pl = \"instanceOf\"\n",
    "        elif p == \"P112\":\n",
    "            pl = \"foundedBy\"\n",
    "        elif p == \"P856\":\n",
    "            pl = \"officialWebsite\"\n",
    "        else:\n",
    "            pl = \"industryType\"\n",
    "\n",
    "        query = (\"#Continents, countries, regions and capitals\\n\"\n",
    "        \"#defaultView:Tree\\n\"\n",
    "        \"SELECT ?valLabel\\n\"\n",
    "        \"WHERE\\n\" \n",
    "        \"{\\n\"\n",
    "        \"wd:\"+x+\" wdt:\"+p+\" ?val.\\n\"\n",
    "        \"SERVICE wikibase:label {\\n\"\n",
    "        'bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\".'\n",
    "        \"}\"\n",
    "        \"}\")\n",
    "\n",
    "        sparql = SPARQLWrapper(endpoint_url, agent = \"ChicoBot Test agent\")\n",
    "        sparql.setQuery(query)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        continent_results = sparql.query().convert()\n",
    "\n",
    "        result = str(continent_results[\"results\"][\"bindings\"])\n",
    "        result = str(result[result.find(\"'value': '\")+len(\"'value': '\"):result.rfind(\"'\")])\n",
    "\n",
    "        if result != \"\":\n",
    "            print(result)\n",
    "        else:\n",
    "            result = '<font color=\"red\"><i>MISSING DATA</i></font>'\n",
    "\n",
    "        for s,p,o in g:\n",
    "            if y in o:\n",
    "                g_expand.add((Literal(s),sch.name,Literal(y)))\n",
    "                g_expand.add((Literal(s),wdt[pl],Literal(result)))\n",
    "    return g_expand\n",
    "\n",
    "\n",
    "def departmentQuery(x,y,g_expand):\n",
    "    from SPARQLWrapper import SPARQLWrapper, JSON, CSV\n",
    "    import json\n",
    "    \n",
    "    for p in [\"P279\",\"P3984\",\"P1365\",\"P2579\",\"P3095\"]:\n",
    "        endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "        \n",
    "        if p == \"P279\":\n",
    "            pl = \"subClassOf\"\n",
    "        elif p == \"P1365\":\n",
    "            pl = \"replaces\"\n",
    "        elif p == \"P2579\":\n",
    "            pl = \"studiedBy\"\n",
    "        elif p == \"P3095\":\n",
    "            pl = \"practicedBy\"\n",
    "        else:\n",
    "            pl = \"subredditName\"\n",
    "\n",
    "        query = (\"#Continents, countries, regions and capitals\\n\"\n",
    "        \"#defaultView:Tree\\n\"\n",
    "        \"SELECT ?valLabel\\n\"\n",
    "        \"WHERE\\n\" \n",
    "        \"{\\n\"\n",
    "        \"wd:\"+x+\" wdt:\"+p+\" ?val.\\n\"\n",
    "        \"SERVICE wikibase:label {\\n\"\n",
    "        'bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\".'\n",
    "        \"}\"\n",
    "        \"}\")\n",
    "\n",
    "        sparql = SPARQLWrapper(endpoint_url, agent = \"ChicoBot Test agent\")\n",
    "        sparql.setQuery(query)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        continent_results = sparql.query().convert()\n",
    "\n",
    "        result = str(continent_results[\"results\"][\"bindings\"])\n",
    "        result = str(result[result.find(\"'value': '\")+len(\"'value': '\"):result.rfind(\"'\")])\n",
    "        \n",
    "        if result != \"\":\n",
    "            print(result)\n",
    "        else:\n",
    "            result = '<font color=\"red\"><i>MISSING DATA</i></font>'\n",
    "\n",
    "        for s,p,o in g:\n",
    "            if y in o:\n",
    "                g_expand.add((Literal(s),sch.department,Literal(y)))\n",
    "                g_expand.add((Literal(s),sch[pl],Literal(result)))\n",
    "    return g_expand\n",
    "\n",
    "def topicQuery(x,y,g_expand):\n",
    "    from SPARQLWrapper import SPARQLWrapper, JSON, CSV\n",
    "    import json\n",
    "    \n",
    "    for p in [\"P1476\",\"P921\",\"2093\",\"P577\", \"P1433\"]:\n",
    "        endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "        \n",
    "        if p == \"P1476\":\n",
    "            pl = \"title\"\n",
    "        if p == \"P921\":\n",
    "            pl = \"mainSubject\"\n",
    "        if p == \"P2093\":\n",
    "            pl = \"authorOfMainSubject\"\n",
    "        if p == \"P577\":\n",
    "            pl = \"publicationDate\"  \n",
    "        else:\n",
    "            pl = \"publishedIn\"\n",
    "\n",
    "        query = (\"#Continents, countries, regions and capitals\\n\"\n",
    "        \"#defaultView:Tree\\n\"\n",
    "        \"SELECT ?valLabel\\n\"\n",
    "        \"WHERE\\n\" \n",
    "        \"{\\n\"\n",
    "        \"wd:\"+x+\" wdt:\"+p+\" ?val.\\n\"\n",
    "        \"SERVICE wikibase:label {\\n\"\n",
    "        'bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\".'\n",
    "        \"}\"\n",
    "        \"}\")\n",
    "\n",
    "        sparql = SPARQLWrapper(endpoint_url, agent = \"ChicoBot Test agent\")\n",
    "        sparql.setQuery(query)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        continent_results = sparql.query().convert()\n",
    "\n",
    "        result = str(continent_results[\"results\"][\"bindings\"])\n",
    "        result = str(result[result.find(\"'value': '\")+len(\"'value': '\"):result.rfind(\"'\")])\n",
    "        \n",
    "        if result != \"\":\n",
    "            print(result)\n",
    "        else:\n",
    "            result = '<font color=\"red\"><i>MISSING DATA</i></font>'\n",
    "\n",
    "            g_expand.add((pr.t1,sch[pl],Literal(result)))\n",
    "    \n",
    "    return g_expand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert WikiData Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q30987748 microrobotics\n",
      "Q14949557 Mark S. Fox\n",
      "male\n",
      "computer scientist\n",
      "University of Toronto\n",
      "1952-05-09T00:00:00Z\n",
      "human\n",
      "Canada\n",
      "Q4662529 Aaron Wheeler\n",
      "male\n",
      "chemist'}}, {'valLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'researcher\n",
      "human\n",
      "Q172 Toronto\n",
      "Ontario\n",
      "2731571\n",
      "John Tory\n",
      "city'}}, {'valLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'single-tier municipality'}}, {'valLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'provincial or territorial capital city in Canada\n",
      "Fort Rouillé\n",
      "Q16 Canada\n",
      "North America\n",
      "Ottawa\n",
      "1867-07-01T00:00:00Z\n",
      "country'}}, {'valLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'sovereign state\n",
      "82.30051\n",
      "Q2931014 CBC News\n",
      "CBC Ottawa Broadcast Centre\n",
      "business\n",
      "mass media\n",
      "Canadian Broadcasting Corporation\n",
      "http://www.cbc.ca/news/\n",
      "Q2329 Chemistry\n",
      "physical science\n",
      "chemistry\n",
      "alchemy\n",
      "engineering\n",
      "chemist\n",
      "Q21198 Computer science\n",
      "formal science\n",
      "compsci\n",
      "computer scientist'}}, {'valLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'IT Instructor\n",
      "MENTION--TYPE--CLASS\n",
      "<b>PERSON MENTION 2</b>--<b>PREDICATE</b>--<b>OBJECT</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p2--https://www.wikidata.org/wiki/Property_talk:educatedAt--University of Toronto\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p2--https://www.wikidata.org/wiki/Property_talk:occupation--computer scientist\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p2--https://www.wikidata.org/wiki/Property_talk:sex--male\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p2--https://www.wikidata.org/wiki/Property_talk:instanceOf--human\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p2--https://www.wikidata.org/wiki/Property_talk:dateOfBirth--1952-05-09T00:00:00Z\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p2--https://www.wikidata.org/wiki/Property_talk:citizenship--Canada\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p2--http://schema.org/person--Mark S. Fox\n",
      "<b>PERSON MENTION 3</b>--<b>PREDICATE</b>--<b>OBJECT</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p3--https://www.wikidata.org/wiki/Property_talk:citizenship--<font color=\"red\"><i>MISSING DATA</i></font>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p3--https://www.wikidata.org/wiki/Property_talk:occupation--chemist'}}, {'valLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'researcher\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p3--https://www.wikidata.org/wiki/Property_talk:instanceOf--human\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p3--https://www.wikidata.org/wiki/Property_talk:dateOfBirth--<font color=\"red\"><i>MISSING DATA</i></font>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p3--https://www.wikidata.org/wiki/Property_talk:sex--male\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p3--http://schema.org/person--Aaron Wheeler\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p3--https://www.wikidata.org/wiki/Property_talk:educatedAt--<font color=\"red\"><i>MISSING DATA</i></font>\n",
      "<b>WEB SOURCE 2</b>--<b>PREDICATE</b>--<b>OBJECT</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#s2--https://www.wikidata.org/wiki/Property_talk:instanceOf--business\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#s2--https://www.wikidata.org/wiki/Property_talk:officialWebsite--http://www.cbc.ca/news/\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#s2--https://www.wikidata.org/wiki/Property_talk:foundedBy--Canadian Broadcasting Corporation\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#s2--https://www.wikidata.org/wiki/Property_talk:headquartersLocatedIn--CBC Ottawa Broadcast Centre\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#s2--http://schema.org/name--CBC News\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#s2--https://www.wikidata.org/wiki/Property_talk:industryType--mass media\n",
      "<b>COUNTRY SOURCE 1</b>--<b>PREDICATE</b>--<b>OBJECT</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#country1--http://schema.org/countryOfOrigin--Canada\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#country1--https://www.wikidata.org/wiki/Property_talk:capitalCity--Ottawa\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#country1--https://www.wikidata.org/wiki/Property_talk:lifeExpectancy--82.30051\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#country1--https://www.wikidata.org/wiki/Property_talk:inceptionDate--1867-07-01T00:00:00Z\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#country1--https://www.wikidata.org/wiki/Property_talk:instanceOf--country'}}, {'valLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'sovereign state\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#country1--https://www.wikidata.org/wiki/Property_talk:continent--North America\n",
      "<b>CITY SOURCE 1</b>--<b>PREDICATE</b>--<b>OBJECT</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#city1--https://www.wikidata.org/wiki/Property_talk:namedAfter--Fort Rouillé\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#city1--https://www.wikidata.org/wiki/Property_talk:cityMayor--John Tory\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#city1--http://schema.org/inCity--Toronto\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#city1--https://www.wikidata.org/wiki/Property_talk:population--2731571\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#city1--https://www.wikidata.org/wiki/Property_talk:capitalOf--Ontario\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#city1--https://www.wikidata.org/wiki/Property_talk:instanceOf--city'}}, {'valLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'single-tier municipality'}}, {'valLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'provincial or territorial capital city in Canada\n",
      "<b>DEPARTMENT 1</b>--<b>PREDICATE</b>--<b>OBJECT</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#d1--http://schema.org/subClassOf--physical science\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#d1--http://schema.org/studiedBy--engineering\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#d1--http://schema.org/replaces--alchemy\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#d1--http://schema.org/practicedBy--chemist\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#d1--http://schema.org/department--Chemistry\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#d1--http://schema.org/subredditName--chemistry\n",
      "<b>DEPARTMENT 2</b>--<b>PREDICATE</b>--<b>OBJECT</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#d2--http://schema.org/subClassOf--formal science\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#d2--http://schema.org/department--Computer science\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#d2--http://schema.org/replaces--<font color=\"red\"><i>MISSING DATA</i></font>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#d2--http://schema.org/studiedBy--<font color=\"red\"><i>MISSING DATA</i></font>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#d2--http://schema.org/subredditName--compsci\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#d2--http://schema.org/practicedBy--computer scientist'}}, {'valLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'IT Instructor\n",
      "<b>TOPIC 1</b>--<b>PREDICATE</b>--<b>OBJECT</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#t1--http://schema.org/publishedIn--<font color=\"red\"><i>MISSING DATA</i></font>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#t1--http://schema.org/publicationDate--<font color=\"red\"><i>MISSING DATA</i></font>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from requests import get\n",
    "\n",
    "instance_uri_list = []\n",
    "instance_list = []\n",
    "source.append(\"CBC News\")\n",
    "\n",
    "instances = [list(set(person)),list(set(city)),list(set(country)),list(set(source)),list(set(department)),list(set(topic_string))]\n",
    "\n",
    "#Get Topic\n",
    "#Was not working, so hard coded.\n",
    "g_expand = Graph()\n",
    "x = \"Q30987748\"\n",
    "a = \"microrobotics\"\n",
    "print(f\"{x} {a}\")\n",
    "topicQuery(a,x,g_expand)\n",
    "\n",
    "for instance in instances:\n",
    "    for x in instance:\n",
    "        try:\n",
    "            resp = get('https://www.wikidata.org/w/api.php', {\n",
    "                'action': 'wbgetentities',\n",
    "                'titles': x,\n",
    "                'sites': 'enwiki',\n",
    "                'props': '',\n",
    "                'format': 'json'\n",
    "            }).json()\n",
    "\n",
    "            count = 0\n",
    "            a = list(resp['entities'])[0]\n",
    "\n",
    "            if instance == list(set(country)) and \"Q\" in a:\n",
    "                print(f\"{a} {x}\")\n",
    "                countryQuery(a,x,g_expand)\n",
    "                count += 1\n",
    "\n",
    "            if instance == list(set(person)) and \"Q\" in a:\n",
    "                print(f\"{a} {x}\")\n",
    "                personQuery(a,x,g_expand)\n",
    "                count += 1\n",
    "\n",
    "            if instance == list(set(department)) and \"Q\" in a:\n",
    "                print(f\"{a} {x}\")\n",
    "                departmentQuery(a,x,g_expand)\n",
    "                count += 1\n",
    "\n",
    "            if instance == list(set(source)) and \"Q\" in a:\n",
    "                print(f\"{a} {x}\")\n",
    "                sourceQuery(a,x,g_expand)\n",
    "                count += 1\n",
    "\n",
    "            if instance == list(set(city)) and \"Q\" in a:\n",
    "                print(f\"{a} {x}\")\n",
    "                cityQuery(a,x,g_expand)\n",
    "                count += 1\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "#Converting knowledge graph to text ---------------------------------------------------------------------------------------\n",
    "kg_list = []\n",
    "master_list = [\"MENTION--TYPE--CLASS\\n\"]\n",
    "\n",
    "for s,p,o in g_expand:\n",
    "    gr = str(s) + \"--\" + str(p) + \"--\" + str(o) + \"\\n\"\n",
    "    kg_list.append(gr)\n",
    "\n",
    "if len(m_list) > 0:\n",
    "    for i in range(1,max(m_list)+1):\n",
    "        if \"p\" + str(i) in str(kg_list):\n",
    "            master_list.append(\"<b>PERSON MENTION \" + str(i) + \"</b>--<b>PREDICATE</b>--<b>OBJECT</b>\\n\")\n",
    "            for kg in kg_list:\n",
    "                if \"p\"+str(i) in kg:\n",
    "                    master_list.append(kg)\n",
    "\n",
    "if len(s_list) > 0:\n",
    "    for i in range(1,max(s_list)+1):\n",
    "        if \"s\" + str(i) in str(kg_list):\n",
    "            master_list.append(\"<b>WEB SOURCE \" + str(i) + \"</b>--<b>PREDICATE</b>--<b>OBJECT</b>\\n\")\n",
    "            for kg in kg_list:\n",
    "                if \"s\"+str(i) in kg:\n",
    "                    master_list.append(kg)\n",
    "                \n",
    "if len(s_list) > 0:\n",
    "    for i in range(1,max(s_list)+1):\n",
    "        if \"country\" + str(i) in str(kg_list):\n",
    "            master_list.append(\"<b>COUNTRY SOURCE \" + str(i) + \"</b>--<b>PREDICATE</b>--<b>OBJECT</b>\\n\")\n",
    "            for kg in kg_list:\n",
    "                if \"country\"+str(i) in kg:\n",
    "                    master_list.append(kg)\n",
    "                \n",
    "if len(s_list) > 0:\n",
    "    for i in range(1,max(s_list)+1):\n",
    "        if \"city\" + str(i) in str(kg_list):\n",
    "            master_list.append(\"<b>CITY SOURCE \" + str(i) + \"</b>--<b>PREDICATE</b>--<b>OBJECT</b>\\n\")\n",
    "            for kg in kg_list:\n",
    "                if \"city\"+str(i) in kg:\n",
    "                    master_list.append(kg)\n",
    "\n",
    "if len(d_list) > 0:\n",
    "    for i in range(1,max(d_list)+1):\n",
    "        if \"d\" + str(i) in str(kg_list):\n",
    "            master_list.append(\"<b>DEPARTMENT \" + str(i) + \"</b>--<b>PREDICATE</b>--<b>OBJECT</b>\\n\")\n",
    "            for kg in kg_list:\n",
    "                if \"d\"+str(i) in kg:\n",
    "                    master_list.append(kg)\n",
    "\n",
    "if \"t1\" in str(kg_list):\n",
    "            master_list.append(\"<b>TOPIC 1</b>--<b>PREDICATE</b>--<b>OBJECT</b>\\n\")\n",
    "            for kg in kg_list:\n",
    "                if \"t1\" in kg:\n",
    "                    master_list.append(kg)\n",
    "\n",
    "gr = ''.join(master_list)\n",
    "print(gr)\n",
    "\n",
    "createExtendedTable(gr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function\n",
    "\n",
    "- Run this to enter URL's into prompt.\n",
    "- Webscrapes article information and additional data.\n",
    "- Primary program for Assignment 1 to call all relevant functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please copy and paste desired news article here: https://news.engineering.utoronto.ca/microrobots-to-change-the-way-we-work-with-cellular-material/\n",
      "\n",
      "Title/ Topic: Microrobots to change the way we work with cellular material - U of T Engineering News\n",
      "\n",
      "Web Source: ['news.engineering.utoronto.ca']\n",
      "\n",
      "Printing Data to be added to Knowledgegraph ------------------------------------\n",
      "\n",
      "Person: {'Dan Haves', 'Shuailong Zhang', 'Mark S. Fox', 'Aaron Wheeler'}\n",
      "\n",
      "City Name: {'Toronto'}\n",
      "Country Name: {'Canada'}\n",
      "Department Name: {'Chemistry', 'Computer science'}\n",
      "\n",
      "Printing Knowledgegraph --------------------------------------------------------\n",
      "\n",
      "MENTION--TYPE--CLASS\n",
      "<b>MENTION</b>--<b>TYPE</b>--<b>CLASS</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Source--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#inCity--http://schema.org/City\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Source--http://schema.org/countryOfOrigin--http://schema.org/Country\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Mention--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#date--http://www.w3.org/2001/XMLSchema#datetime\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Mention--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#person--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Person\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Department--http://schema.org/description--http://www.w3.org/2001/XMLSchema#string\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Mention--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#source--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Source\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Source--http://schema.org/name--http://www.w3.org/2001/XMLSchema#string\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Mention--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#topic--http://www.w3.org/2001/XMLSchema#string\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Mention--http://www.w3.org/1999/02/22-rdf-syntax-ns#type--http://www.w3.org/2002/07/owl#Class\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Department--http://schema.org/name--http://www.w3.org/2001/XMLSchema#string\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Department--http://www.w3.org/2000/01/rdf-schema#subClassOf--http://schema.org/Organization\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Source--http://schema.org/description--http://www.w3.org/2001/XMLSchema#string\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Mention--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#department--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Department\n",
      "<b>PERSON 1</b>--<b>PREDICATE</b>--<b>OBJECT</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Person--http://www.w3.org/1999/02/22-rdf-syntax-ns#type--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p1\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p1--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#name--Dan Haves\n",
      "<b>PERSON 2</b>--<b>PREDICATE</b>--<b>OBJECT</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Person--http://www.w3.org/1999/02/22-rdf-syntax-ns#type--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p2\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p2--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#name--Mark S. Fox\n",
      "<b>PERSON 3</b>--<b>PREDICATE</b>--<b>OBJECT</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Person--http://www.w3.org/1999/02/22-rdf-syntax-ns#type--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p3\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p3--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#name--Aaron Wheeler\n",
      "<b>PERSON 4</b>--<b>PREDICATE</b>--<b>OBJECT</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p4--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#name--Shuailong Zhang\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Person--http://www.w3.org/1999/02/22-rdf-syntax-ns#type--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#p4\n",
      "<b>CITY SOURCE 1</b>--<b>PREDICATE</b>--<b>OBJECT</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#City--http://www.w3.org/1999/02/22-rdf-syntax-ns#type--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#city1\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#city1--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#name--Toronto\n",
      "<b>CITY SOURCE 2</b>--<b>PREDICATE</b>--<b>OBJECT</b>\n",
      "<b>COUNTRY SOURCE 1</b>--<b>PREDICATE</b>--<b>WEB SOURCE</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#country1--http://schema.org/name--Canada\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Country--http://www.w3.org/1999/02/22-rdf-syntax-ns#type--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#country1\n",
      "<b>COUNTRY SOURCE 2</b>--<b>PREDICATE</b>--<b>WEB SOURCE</b>\n",
      "<b>WEB SOURCE 1</b>--<b>PREDICATE</b>--<b>WEB SOURCE</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#s1--http://schema.org/name--['news.engineering.utoronto.ca']\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Source--http://www.w3.org/1999/02/22-rdf-syntax-ns#type--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#s1\n",
      "<b>WEB SOURCE 2</b>--<b>PREDICATE</b>--<b>WEB SOURCE</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#s2--http://schema.org/name--CBC News\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Source--http://www.w3.org/1999/02/22-rdf-syntax-ns#type--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#s2\n",
      "<b>DEPARTMENT 1</b>--<b>PREDCIATE</b>--<b>ORGANIZATION</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Department--http://www.w3.org/1999/02/22-rdf-syntax-ns#type--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#d1\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#d1--http://schema.org/name--Chemistry\n",
      "<b>DEPARTMENT 2</b>--<b>PREDCIATE</b>--<b>ORGANIZATION</b>\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#d2--http://schema.org/name--Computer science\n",
      "http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#Department--http://www.w3.org/1999/02/22-rdf-syntax-ns#type--http://ontology.eil.utoronto.ca/MIE1501/publicity.owl#d2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try:    \n",
    "\n",
    "m_list = []\n",
    "s_list = []\n",
    "d_list = []\n",
    "\n",
    "# Prompts user to enter web article\n",
    "press_article = input(\"Please copy and paste desired news article here: \")\n",
    "\n",
    "g = Graph()\n",
    "nlp = sp.load(\"en_core_web_sm\")\n",
    "\n",
    "#Website to be parsed\n",
    "web_url = (press_article)\n",
    "\n",
    "#Webscraping Title of Article\n",
    "ttl_data = urllib.request.urlopen(web_url).read()\n",
    "ttl_soup = BeautifulSoup(ttl_data,'html.parser')\n",
    "ttl_text = str(ttl_soup.find_all('title'))\n",
    "\n",
    "#Webscraping Article URL\n",
    "data = urllib.request.urlopen(web_url).read()\n",
    "soup = BeautifulSoup(data,'html.parser')\n",
    "text = str(soup.find_all('p'))\n",
    "\n",
    "#Webscraping Department URL\n",
    "#Checks to see if department is listed at UofT\n",
    "#Website may be temporarily down\n",
    "dep_url = \"https://www.utoronto.ca/a-to-z-directory\"\n",
    "dep_data = urllib.request.urlopen(dep_url).read()\n",
    "dep_soup = BeautifulSoup(dep_data,'html.parser')\n",
    "dep_text = str(dep_soup.find_all('a'))\n",
    "\n",
    "# Webscraping Countries URL\n",
    "# If GPE label in entity, check to see if label is country\n",
    "cty_url = \"https://www.worldometers.info/geography/alphabetical-list-of-countries/\"\n",
    "cty_data = urllib.request.urlopen(cty_url).read()\n",
    "cty_soup = BeautifulSoup(cty_data,'html.parser')\n",
    "cty_text = str(cty_soup.find_all('td'))\n",
    "\n",
    "text.split(\"</p>\")\n",
    "count = 0\n",
    "n = 0\n",
    "m = \"m\"\n",
    "\n",
    "#A list of words that should be neglected from function departmentSearch and looked up in the article\n",
    "null_words = [\"UofT\",\"uoft\",\"U Of T\", \"U of T\",\"The University of Toronto\",\n",
    "              \"the University of Toronto\", \"University of Toronto\",\"university of toronto\", \"utoronto\"]\n",
    "\n",
    "#A list of months\n",
    "date_words = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\n",
    "                                                                                 \"November\",\"December\"]\n",
    "\n",
    "#A list of numbers that may appear in a string\n",
    "num_words = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "\n",
    "#Create list of variables that will be appended in each iteration\n",
    "person = []\n",
    "city = []\n",
    "country = []\n",
    "datetime = []\n",
    "department = []\n",
    "source = []\n",
    "\n",
    "if any(k in text for k in null_words):\n",
    "\n",
    "    # Extracts title (Topic String)\n",
    "    topic_string = ttl_text.split('<title>')[-1].split('</title>')[0]\n",
    "    print(f\"\\nTitle/ Topic: {topic_string}\")\n",
    "\n",
    "    # Extracts Webpage Source\n",
    "    source.append(web_url.split('//')[-1].split('/')[0])\n",
    "    print (f\"\\nWeb Source: {source}\")\n",
    "\n",
    "    #Checks to see if UofT is mentioned in title or web URL.\n",
    "    #This will indicate if entire article is relevant or not\n",
    "    if any(k in topic_string for k in null_words) or any(k in web_url for k in null_words):\n",
    "\n",
    "        for token in nlp(text).ents:\n",
    "            facultySearch(token) #Person\n",
    "            dateSearch(token) # Date\n",
    "            citySearch(token) # City\n",
    "            countrySearch(token) # Country\n",
    "            departmentSearch(token)  # Department\n",
    "\n",
    "    #If uoft is not mentioned in title or url, it will check each paragraph to see if a mention can be found.\n",
    "    for line in text.split(\"</p>\"):\n",
    "        for token in nlp(line).ents:\n",
    "            if any(k in line for k in null_words):\n",
    "                facultySearch(token) #Person\n",
    "                dateSearch(token) # Date\n",
    "                citySearch(token) # City\n",
    "                countrySearch(token) # Country\n",
    "                departmentSearch(token)  # Department\n",
    "\n",
    "    print(\"\\nPrinting Data to be added to Knowledgegraph ------------------------------------\\n\")\n",
    "\n",
    "    #Prints and summarizes data in a formatted list to make it easier to read\n",
    "    if not person:\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"Person: {set(person)}\")\n",
    "\n",
    "    if not datetime:\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"Date: {set(datetime)}\")\n",
    "    if not city:\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"City Name: {set(city)}\")\n",
    "    if not country:\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"Country Name: {set(country)}\")\n",
    "    if not department:\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"Department Name: {set(department)}\")\n",
    "\n",
    "    print(\"\\nPrinting Knowledgegraph --------------------------------------------------------\\n\")\n",
    "    department = [dept.capitalize() for dept in department]\n",
    "    addObject()\n",
    "\n",
    "else:\n",
    "    #Print if UofT mention cannot be found in article\n",
    "    print(\"No mention of UofT\")\n",
    "# except:\n",
    "#     print(\"Unrecognized URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_a2 = Graph()\n",
    "g_a2 = (g + g_expand)\n",
    "g_a2.serialize(destination = \"Assignment 2 Triples\", format = 'turtle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
